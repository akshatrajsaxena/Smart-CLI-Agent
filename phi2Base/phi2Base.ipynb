{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb136d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHI-2 BASE MODEL EVALUATION\n",
      "============================================================\n",
      "Using device: cpu\n",
      "Loading Phi-2 model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94f394853d146868ea9272a22b3aa73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device: cpu\n",
      "\n",
      "Evaluating on test prompts...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Test 1/7: Create a new Git branch and switch to it.\n",
      "Reference: git checkout -b new_branch\n",
      "Generated: To create a new branch, run the following command in your terminal or command prompt:\n",
      "```python\n",
      "git checkout -b <branch_name>\n",
      "```\n",
      "Replace `<branch_name>` with any name you want for your new branch. To switch to the new branch, use the same command again:\n",
      "BLEU: 0.021\n",
      "ROUGE-L: 0.204\n",
      "Command Accuracy: 0.000\n",
      "Plan Quality: 1/2\n",
      "\n",
      "Test 2/7: Compress the folder reports into reports.tar.gz.\n",
      "Reference: tar -czf reports.tar.gz reports/\n",
      "Generated: To compress a folder, we can use the tar command with the -z option to create a gzip compressed file.\n",
      "```python\n",
      "!tar zcf /path/to/folder/reports/*.txt.gz\n",
      "```\n",
      "This will create a new file called reports.tar.gz in the current directory containing all.txt files in the given folder and their contents are compressed using gzip compression.\n",
      "BLEU: 0.004\n",
      "ROUGE-L: 0.123\n",
      "Command Accuracy: 0.000\n",
      "Plan Quality: 1/2\n",
      "\n",
      "Test 3/7: List all Python files in the current directory recursively.\n",
      "Reference: find . -name '*.py' -type f\n",
      "Generated: ```python\n",
      "for file_name in os.listdir():\n",
      "if file_name.endswith('.py'):\n",
      "print(file_name)\n",
      "```\n",
      "BLEU: 0.000\n",
      "ROUGE-L: 0.200\n",
      "Command Accuracy: 0.000\n",
      "Plan Quality: 1/2\n",
      "\n",
      "Test 4/7: Set up a virtual environment and install requests.\n",
      "Reference: python -m venv myenv && source myenv/bin/activate && pip install requests\n",
      "Generated: To set up a virtual environment, you can use the command \"python -m venv myenv\". This will create a new folder called \"myenv\" with all the necessary files for your Python project. To activate the environment, run the command \"source myenv/bin/activate\". Then, to install requests, run the command \"pip install requests\".\n",
      "## Exercise 1: Write a function that sends an HTTP GET request to a given URL and returns the response content as text.\n",
      "```python\n",
      "import requests\n",
      "def get_response(url):\n",
      "BLEU: 0.006\n",
      "ROUGE-L: 0.237\n",
      "Command Accuracy: 0.000\n",
      "Plan Quality: 2/2\n",
      "\n",
      "Test 5/7: Fetch only the first ten lines of a file named output.log.\n",
      "Reference: head -n 10 output.log\n",
      "Generated: ```python\n",
      "with open(\"output.log\") as f:\n",
      "for i, line in enumerate(f):\n",
      "if i >= 10:\n",
      "break\n",
      "BLEU: 0.000\n",
      "ROUGE-L: 0.182\n",
      "Command Accuracy: 0.000\n",
      "Plan Quality: 1/2\n",
      "\n",
      "Test 6/7: How do I find and replace text in multiple files using command line?\n",
      "Reference: find . -type f -exec sed -i 's/oldtext/newtext/g' {} +\n",
      "Generated: To find and replace text in multiple files using command line, you can use the following code:\n",
      "```python\n",
      "import glob\n",
      "for file_name in glob.glob('*.txt'):\n",
      "with open(file_name) as f:\n",
      "BLEU: 0.007\n",
      "ROUGE-L: 0.093\n",
      "Command Accuracy: 0.000\n",
      "Plan Quality: 1/2\n",
      "\n",
      "Test 7/7: What command should I use to monitor real-time system processes and memory usage?\n",
      "Reference: top -o %MEM\n",
      "Generated: Use the psutil module's process_iter() function to iterate over all running processes. Then, for each process, you can use its methods like cpu_percent(), memory_info(), etc., to get information about its CPU usage and memory usage.\n",
      "```python\n",
      "import psutil\n",
      "for proc in psutil.process_iter():\n",
      "try:\n",
      "BLEU: 0.000\n",
      "ROUGE-L: 0.000\n",
      "Command Accuracy: 0.000\n",
      "Plan Quality: 2/2\n",
      "\n",
      "============================================================\n",
      "EVALUATION SUMMARY\n",
      "============================================================\n",
      "Average BLEU Score: 0.005\n",
      "Average ROUGE-L Score: 0.148\n",
      "Average Command Accuracy: 0.000\n",
      "Average Plan Quality: 1.286/2.0\n",
      "\n",
      "Detailed results saved to: logs/phi2_base_evaluation.json\n",
      "\n",
      "Evaluation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datetime import datetime\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    \"Create a new Git branch and switch to it.\",\n",
    "    \"Compress the folder reports into reports.tar.gz.\",\n",
    "    \"List all Python files in the current directory recursively.\",\n",
    "    \"Set up a virtual environment and install requests.\",\n",
    "    \"Fetch only the first ten lines of a file named output.log.\",\n",
    "    \"How do I find and replace text in multiple files using command line?\",\n",
    "    \"What command should I use to monitor real-time system processes and memory usage?\"\n",
    "]\n",
    "\n",
    "\n",
    "REFERENCE_ANSWERS = [\n",
    "    \"git checkout -b new_branch\",\n",
    "    \"tar -czf reports.tar.gz reports/\",\n",
    "    \"find . -name '*.py' -type f\",\n",
    "    \"python -m venv myenv && source myenv/bin/activate && pip install requests\",\n",
    "    \"head -n 10 output.log\",\n",
    "    \"find . -type f -exec sed -i 's/oldtext/newtext/g' {} +\",\n",
    "    \"top -o %MEM\"\n",
    "]\n",
    "\n",
    "class MetricsCalculator:    \n",
    "    def __init__(self):\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "        self.smoothing_function = SmoothingFunction().method1\n",
    "\n",
    "    def calculate_bleu(self, reference, candidate):\n",
    "        if not reference or not candidate:\n",
    "            return 0.0\n",
    "        \n",
    "        reference_tokens = reference.lower().split()\n",
    "        candidate_tokens = candidate.lower().split()        \n",
    "        try:\n",
    "            score = sentence_bleu([reference_tokens], candidate_tokens,smoothing_function=self.smoothing_function)\n",
    "            return score\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def calculate_rouge_l(self, reference, candidate):\n",
    "        if not reference or not candidate:\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            scores = self.rouge_scorer.score(reference, candidate)\n",
    "            return scores['rougeL'].fmeasure\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def calculate_command_accuracy(self, reference, candidate):\n",
    "        if not reference or not candidate:\n",
    "            return 0.0\n",
    "        ref_commands = self.extract_commands(reference)\n",
    "        cand_commands = self.extract_commands(candidate)\n",
    "        \n",
    "        if not ref_commands and not cand_commands:\n",
    "            return 1.0  \n",
    "        if not ref_commands or not cand_commands:\n",
    "            return 0.0  \n",
    "        \n",
    "        ref_main = ref_commands[0] if ref_commands else \"\"\n",
    "        cand_main = cand_commands[0] if cand_commands else \"\"\n",
    "        \n",
    "        if ref_main.lower() in cand_main.lower() or cand_main.lower() in ref_main.lower():\n",
    "            return 1.0\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    def extract_commands(self, text):\n",
    "        command_pattern = r'\\b(?:git|tar|find|python|pip|head|top|ls|cd|cp|mv|rm|mkdir|chmod|grep|sed|awk)\\b[^\\n]*'\n",
    "        commands = re.findall(command_pattern, text.lower())\n",
    "        return commands\n",
    "    \n",
    "    def score_plan_quality(self, prompt, response):\n",
    "        if not response:\n",
    "            return 0\n",
    "        response_lower = response.lower()\n",
    "        has_command = bool(re.search(r'\\b(?:git|tar|find|python|pip|head|top|ls|cd|cp|mv|rm|mkdir|chmod|grep|sed|awk)\\b', response_lower))\n",
    "        has_steps = bool(re.search(r'\\b(?:step|first|then|next|finally|\\d+\\.)\\b', response_lower))\n",
    "        prompt_lower = prompt.lower()\n",
    "        relevant_keywords = []\n",
    "        if \"git\" in prompt_lower:\n",
    "            relevant_keywords = [\"git\", \"branch\", \"checkout\"]\n",
    "        elif \"compress\" in prompt_lower or \"tar\" in prompt_lower:\n",
    "            relevant_keywords = [\"tar\", \"compress\", \"gz\"]\n",
    "        elif \"python\" in prompt_lower and \"files\" in prompt_lower:\n",
    "            relevant_keywords = [\"find\", \"python\", \"*.py\"]\n",
    "        elif \"virtual environment\" in prompt_lower:\n",
    "            relevant_keywords = [\"venv\", \"pip\", \"install\"]\n",
    "        elif \"lines\" in prompt_lower and \"file\" in prompt_lower:\n",
    "            relevant_keywords = [\"head\", \"lines\"]\n",
    "        elif \"find\" in prompt_lower and \"replace\" in prompt_lower:\n",
    "            relevant_keywords = [\"sed\", \"find\", \"replace\"]\n",
    "        elif \"monitor\" in prompt_lower and \"process\" in prompt_lower:\n",
    "            relevant_keywords = [\"top\", \"ps\", \"monitor\"]\n",
    "        \n",
    "        has_relevant = any(keyword in response_lower for keyword in relevant_keywords)\n",
    "        \n",
    "        \n",
    "        if has_command and has_relevant:\n",
    "            if has_steps:\n",
    "                return 2  \n",
    "            else:\n",
    "                return 1  \n",
    "        elif has_command or has_relevant:\n",
    "            return 1  \n",
    "        else:\n",
    "            return 0  \n",
    "\n",
    "def load_phi2_model():\n",
    "    print(\"Loading Phi-2 model and tokenizer...\")    \n",
    "    model_name = \"microsoft/phi-2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name,torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,device_map=\"auto\" if torch.cuda.is_available() else None,trust_remote_code=True)\n",
    "    \n",
    "    print(f\"Model loaded on device: {next(model.parameters()).device}\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def format_prompt(instruction):\n",
    "    return f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "\n",
    "def generate_response(model, tokenizer, prompt, max_new_tokens=150): # limiting the output to 150 token only and omitting repitions\n",
    "    formatted_prompt = format_prompt(prompt)\n",
    "    \n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs,max_new_tokens=max_new_tokens,do_sample=True,temperature=0.7,top_p=0.9,repetition_penalty=1.1,pad_token_id=tokenizer.eos_token_id,eos_token_id=tokenizer.eos_token_id,)\n",
    "    \n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    response_tokens = outputs[0][input_length:]\n",
    "    response = tokenizer.decode(response_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    response = response.strip()\n",
    "    \n",
    "    \n",
    "    lines = response.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        if line.strip() and (not cleaned_lines or line.strip() != cleaned_lines[-1]):\n",
    "            cleaned_lines.append(line.strip())\n",
    "    \n",
    "    return '\\n'.join(cleaned_lines[:5])  \n",
    "\n",
    "def evaluate_model():\n",
    "    print(\"PHI-2 BASE MODEL EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    model, tokenizer = load_phi2_model()\n",
    "    metrics_calc = MetricsCalculator()\n",
    "    results = []\n",
    "    total_bleu = 0.0\n",
    "    total_rouge = 0.0\n",
    "    total_command_acc = 0.0\n",
    "    total_plan_quality = 0\n",
    "    \n",
    "    print(\"\\nEvaluating on test prompts...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, (prompt, reference) in enumerate(zip(TEST_PROMPTS, REFERENCE_ANSWERS), 1):\n",
    "        print(f\"\\nTest {i}/7: {prompt}\")\n",
    "        print(f\"Reference: {reference}\")\n",
    "        try:\n",
    "            response = generate_response(model, tokenizer, prompt)\n",
    "            print(f\"Generated: {response}\")\n",
    "            bleu_score = metrics_calc.calculate_bleu(reference, response)\n",
    "            rouge_score = metrics_calc.calculate_rouge_l(reference, response)\n",
    "            command_acc = metrics_calc.calculate_command_accuracy(reference, response)\n",
    "            plan_quality = metrics_calc.score_plan_quality(prompt, response)\n",
    "            print(f\"BLEU: {bleu_score:.3f}\")\n",
    "            print(f\"ROUGE-L: {rouge_score:.3f}\")\n",
    "            print(f\"Command Accuracy: {command_acc:.3f}\")\n",
    "            print(f\"Plan Quality: {plan_quality}/2\")\n",
    "            \n",
    "            \n",
    "            result = {\"prompt_id\": i,\"prompt\": prompt,\"reference_answer\": reference,\"generated_response\": response,\"metrics\": {\"bleu_score\": bleu_score,\"rouge_l_score\": rouge_score,\"command_accuracy\": command_acc,\"plan_quality\": plan_quality},\"timestamp\": datetime.now().isoformat()}\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            total_bleu += bleu_score\n",
    "            total_rouge += rouge_score\n",
    "            total_command_acc += command_acc\n",
    "            total_plan_quality += plan_quality\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            result = {\n",
    "                \"prompt_id\": i,\n",
    "                \"prompt\": prompt,\n",
    "                \"reference_answer\": reference,\n",
    "                \"generated_response\": f\"ERROR: {str(e)}\",\n",
    "                \"metrics\": {\n",
    "                    \"bleu_score\": 0.0,\n",
    "                    \"rouge_l_score\": 0.0,\n",
    "                    \"command_accuracy\": 0.0,\n",
    "                    \"plan_quality\": 0\n",
    "                },\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            results.append(result)\n",
    "    \n",
    "    num_prompts = len(TEST_PROMPTS)\n",
    "    avg_bleu = total_bleu / num_prompts\n",
    "    avg_rouge = total_rouge / num_prompts\n",
    "    avg_command_acc = total_command_acc / num_prompts\n",
    "    avg_plan_quality = total_plan_quality / num_prompts\n",
    "    \n",
    "    summary = { \"model_name\": \"microsoft/phi-2\", \"evaluation_date\": datetime.now().isoformat(), \"num_test_prompts\": num_prompts, \"average_metrics\": { \"bleu_score\": avg_bleu, \"rouge_l_score\": avg_rouge, \"command_accuracy\": avg_command_acc, \"plan_quality\": avg_plan_quality }, \"detailed_results\": results }\n",
    "    \n",
    "    with open(\"logs/phi2_base_evaluation.json\", \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Average BLEU Score: {avg_bleu:.3f}\")\n",
    "    print(f\"Average ROUGE-L Score: {avg_rouge:.3f}\")\n",
    "    print(f\"Average Command Accuracy: {avg_command_acc:.3f}\")\n",
    "    print(f\"Average Plan Quality: {avg_plan_quality:.3f}/2.0\")\n",
    "    print(f\"\\nDetailed results saved to: logs/phi2_base_evaluation.json\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        summary = evaluate_model()\n",
    "        print(\"\\nEvaluation completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
