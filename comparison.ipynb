{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4968ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison report saved to model_comparison.md\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Download NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "class MetricsCalculator:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "        self.smoothing_function = SmoothingFunction().method1\n",
    "\n",
    "    def calculate_bleu(self, reference, candidate):\n",
    "        if not reference or not candidate:\n",
    "            return 0.0\n",
    "\n",
    "        reference_tokens = reference.lower().split()\n",
    "        candidate_tokens = candidate.lower().split()\n",
    "\n",
    "        try:\n",
    "            score = sentence_bleu(\n",
    "                [reference_tokens],\n",
    "                candidate_tokens,\n",
    "                smoothing_function=self.smoothing_function\n",
    "            )\n",
    "            return score\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    def calculate_rouge_l(self, reference, candidate):\n",
    "        if not reference or not candidate:\n",
    "            return 0.0\n",
    "\n",
    "        try:\n",
    "            scores = self.rouge_scorer.score(reference, candidate)\n",
    "            return scores['rougeL'].fmeasure\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    def calculate_command_accuracy(self, reference, candidate):\n",
    "        if not reference or not candidate:\n",
    "            return 0.0\n",
    "\n",
    "        ref_commands = self.extract_commands(reference)\n",
    "        cand_commands = self.extract_commands(candidate)\n",
    "\n",
    "        if not ref_commands and not cand_commands:\n",
    "            return 1.0\n",
    "        if not ref_commands or not cand_commands:\n",
    "            return 0.0\n",
    "\n",
    "        ref_main = ref_commands[0] if ref_commands else \"\"\n",
    "        cand_main = cand_commands[0] if cand_commands else \"\"\n",
    "\n",
    "        if ref_main.lower() in cand_main.lower() or cand_main.lower() in ref_main.lower():\n",
    "            return 1.0\n",
    "\n",
    "        return 0.0\n",
    "\n",
    "    def extract_commands(self, text):\n",
    "        import re\n",
    "        command_pattern = r'\\b(?:git|tar|find|python|pip|head|top|ls|cd|cp|mv|rm|mkdir|chmod|grep|sed|awk)\\b[^\\n]*'\n",
    "        commands = re.findall(command_pattern, text.lower())\n",
    "        return commands\n",
    "\n",
    "    def score_plan_quality(self, prompt, response):\n",
    "        if not response:\n",
    "            return 0\n",
    "\n",
    "        response_lower = response.lower()\n",
    "\n",
    "        has_command = bool(re.search(r'\\b(?:git|tar|find|python|pip|head|top|ls|cd|cp|mv|rm|mkdir|chmod|grep|sed|awk)\\b', response_lower))\n",
    "        has_steps = bool(re.search(r'\\b(?:step|first|then|next|finally|\\d+\\.)\\b', response_lower))\n",
    "\n",
    "        prompt_lower = prompt.lower()\n",
    "        relevant_keywords = []\n",
    "\n",
    "        if \"git\" in prompt_lower:\n",
    "            relevant_keywords = [\"git\", \"branch\", \"checkout\"]\n",
    "        elif \"compress\" in prompt_lower or \"tar\" in prompt_lower:\n",
    "            relevant_keywords = [\"tar\", \"compress\", \"gz\"]\n",
    "        elif \"python\" in prompt_lower and \"files\" in prompt_lower:\n",
    "            relevant_keywords = [\"find\", \"python\", \"*.py\"]\n",
    "        elif \"virtual environment\" in prompt_lower:\n",
    "            relevant_keywords = [\"venv\", \"pip\", \"install\"]\n",
    "        elif \"lines\" in prompt_lower and \"file\" in prompt_lower:\n",
    "            relevant_keywords = [\"head\", \"lines\"]\n",
    "        elif \"find\" in prompt_lower and \"replace\" in prompt_lower:\n",
    "            relevant_keywords = [\"sed\", \"find\", \"replace\"]\n",
    "        elif \"monitor\" in prompt_lower and \"process\" in prompt_lower:\n",
    "            relevant_keywords = [\"top\", \"ps\", \"monitor\"]\n",
    "\n",
    "        has_relevant = any(keyword in response_lower for keyword in relevant_keywords)\n",
    "\n",
    "        if has_command and has_relevant:\n",
    "            if has_steps:\n",
    "                return 2\n",
    "            else:\n",
    "                return 1\n",
    "        elif has_command or has_relevant:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "def compare_models(base_data, finetuned_data):\n",
    "    metrics_calc = MetricsCalculator()\n",
    "    comparison_results = []\n",
    "\n",
    "    base_results = {r['prompt']: r for r in base_data['detailed_results']}\n",
    "    finetuned_results = {r['prompt']: r for r in finetuned_data['detailed_results']}\n",
    "\n",
    "    for prompt in base_results:\n",
    "        if prompt not in finetuned_results:\n",
    "            continue\n",
    "\n",
    "        base_result = base_results[prompt]\n",
    "        finetuned_result = finetuned_results[prompt]\n",
    "        reference = base_result['reference_answer']\n",
    "\n",
    "        base_response = base_result['generated_response']\n",
    "        finetuned_response = finetuned_result['generated_response']\n",
    "\n",
    "        base_bleu = metrics_calc.calculate_bleu(reference, base_response)\n",
    "        base_rouge = metrics_calc.calculate_rouge_l(reference, base_response)\n",
    "        base_command_acc = metrics_calc.calculate_command_accuracy(reference, base_response)\n",
    "        base_plan_quality = metrics_calc.score_plan_quality(prompt, base_response)\n",
    "\n",
    "        finetuned_bleu = metrics_calc.calculate_bleu(reference, finetuned_response)\n",
    "        finetuned_rouge = metrics_calc.calculate_rouge_l(reference, finetuned_response)\n",
    "        finetuned_command_acc = metrics_calc.calculate_command_accuracy(reference, finetuned_response)\n",
    "        finetuned_plan_quality = metrics_calc.score_plan_quality(prompt, finetuned_response)\n",
    "\n",
    "        comparison_results.append({\n",
    "            'prompt_id': base_result['prompt_id'],\n",
    "            'prompt': prompt,\n",
    "            'reference_answer': reference,\n",
    "            'base_response': base_response,\n",
    "            'finetuned_response': finetuned_response,\n",
    "            'base_metrics': {\n",
    "                'bleu_score': base_bleu,\n",
    "                'rouge_l_score': base_rouge,\n",
    "                'command_accuracy': base_command_acc,\n",
    "                'plan_quality': base_plan_quality\n",
    "            },\n",
    "            'finetuned_metrics': {\n",
    "                'bleu_score': finetuned_bleu,\n",
    "                'rouge_l_score': finetuned_rouge,\n",
    "                'command_accuracy': finetuned_command_acc,\n",
    "                'plan_quality': finetuned_plan_quality\n",
    "            },\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "    base_avg_metrics = {\n",
    "        'bleu_score': sum(r['base_metrics']['bleu_score'] for r in comparison_results) / len(comparison_results),\n",
    "        'rouge_l_score': sum(r['base_metrics']['rouge_l_score'] for r in comparison_results) / len(comparison_results),\n",
    "        'command_accuracy': sum(r['base_metrics']['command_accuracy'] for r in comparison_results) / len(comparison_results),\n",
    "        'plan_quality': sum(r['base_metrics']['plan_quality'] for r in comparison_results) / len(comparison_results)\n",
    "    }\n",
    "\n",
    "    finetuned_avg_metrics = {\n",
    "        'bleu_score': sum(r['finetuned_metrics']['bleu_score'] for r in comparison_results) / len(comparison_results),\n",
    "        'rouge_l_score': sum(r['finetuned_metrics']['rouge_l_score'] for r in comparison_results) / len(comparison_results),\n",
    "        'command_accuracy': sum(r['finetuned_metrics']['command_accuracy'] for r in comparison_results) / len(comparison_results),\n",
    "        'plan_quality': sum(r['finetuned_metrics']['plan_quality'] for r in comparison_results) / len(comparison_results)\n",
    "    }\n",
    "\n",
    "    return comparison_results, base_avg_metrics, finetuned_avg_metrics\n",
    "\n",
    "def generate_markdown_report(comparison_results, base_avg_metrics, finetuned_avg_metrics):\n",
    "    markdown_content = \"# Model Comparison Report\\n\\n\"\n",
    "    markdown_content += f\"**Comparison Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n",
    "    markdown_content += \"## Overview\\n\\n\"\n",
    "    markdown_content += \"This report compares the performance of the base `microsoft/phi-2` model and the fine-tuned `microsoft/phi-2 (fine-tuned with LoRA)` model on seven test prompts, including five standard prompts and two edge cases. Metrics include BLEU, ROUGE-L, command accuracy, and plan quality (scored 0-2).\\n\\n\"\n",
    "\n",
    "    markdown_content += \"## Average Metrics\\n\\n\"\n",
    "    markdown_content += \"| Metric | Base Model | Fine-Tuned Model |\\n\"\n",
    "    markdown_content += \"|--------|------------|------------------|\\n\"\n",
    "    markdown_content += f\"| BLEU Score | {base_avg_metrics['bleu_score']:.3f} | {finetuned_avg_metrics['bleu_score']:.3f} |\\n\"\n",
    "    markdown_content += f\"| ROUGE-L Score | {base_avg_metrics['rouge_l_score']:.3f} | {finetuned_avg_metrics['rouge_l_score']:.3f} |\\n\"\n",
    "    markdown_content += f\"| Command Accuracy | {base_avg_metrics['command_accuracy']:.3f} | {finetuned_avg_metrics['command_accuracy']:.3f} |\\n\"\n",
    "    markdown_content += f\"| Plan Quality | {base_avg_metrics['plan_quality']:.3f} | {finetuned_avg_metrics['plan_quality']:.3f} |\\n\\n\"\n",
    "\n",
    "    markdown_content += \"## Detailed Comparison\\n\\n\"\n",
    "    for result in comparison_results:\n",
    "        markdown_content += f\"### Prompt {result['prompt_id']}: {result['prompt']}\\n\\n\"\n",
    "        markdown_content += f\"**Reference Answer**: `{result['reference_answer']}`\\n\\n\"\n",
    "        markdown_content += \"#### Base Model\\n\"\n",
    "        markdown_content += f\"- **Response**: `{result['base_response']}`\\n\"\n",
    "        markdown_content += f\"- **BLEU Score**: {result['base_metrics']['bleu_score']:.3f}\\n\"\n",
    "        markdown_content += f\"- **ROUGE-L Score**: {result['base_metrics']['rouge_l_score']:.3f}\\n\"\n",
    "        markdown_content += f\"- **Command Accuracy**: {result['base_metrics']['command_accuracy']:.3f}\\n\"\n",
    "        markdown_content += f\"- **Plan Quality**: {result['base_metrics']['plan_quality']}/2\\n\\n\"\n",
    "        markdown_content += \"#### Fine-Tuned Model\\n\"\n",
    "        markdown_content += f\"- **Response**: `{result['finetuned_response']}`\\n\"\n",
    "        markdown_content += f\"- **BLEU Score**: {result['finetuned_metrics']['bleu_score']:.3f}\\n\"\n",
    "        markdown_content += f\"- **ROUGE-L Score**: {result['finetuned_metrics']['rouge_l_score']:.3f}\\n\"\n",
    "        markdown_content += f\"- **Command Accuracy**: {result['finetuned_metrics']['command_accuracy']:.3f}\\n\"\n",
    "        markdown_content += f\"- **Plan Quality**: {result['finetuned_metrics']['plan_quality']}/2\\n\\n\"\n",
    "\n",
    "    return markdown_content\n",
    "\n",
    "def main():\n",
    "    with open(\"./phi2Base/eval_static.md\", \"r\") as f:\n",
    "        base_data = json.load(f)\n",
    "    with open(\"./phi2fineTuned/logs/phi2_finetuned_evaluation.json\", \"r\") as f:\n",
    "        finetuned_data = json.load(f)\n",
    "\n",
    "\n",
    "    comparison_results, base_avg_metrics, finetuned_avg_metrics = compare_models(base_data, finetuned_data)\n",
    "    markdown_content = generate_markdown_report(comparison_results, base_avg_metrics, finetuned_avg_metrics)\n",
    "    with open(\"model_comparison.md\", \"w\") as f:\n",
    "        f.write(markdown_content)\n",
    "\n",
    "    print(\"Comparison report saved to model_comparison.md\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
